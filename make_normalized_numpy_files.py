""" When called directly with correct vars set in the __name__ == '__main__' block, this file processes raw hyperspectral
 files into normalized/calibrated numpy files. """
import numpy as np
import os
from pathlib import Path
import arrow
from enviratron_filename_parser import EnviratronFileNameParser


def get_reference_light_averages(rw_filepath=None, width=1024, height=30, bands=56):
    """ Takes a file containing light hyperspectral ("white") reference data generated by running a hyperspectral
    line scan of a white reference card.
    Returns an array of length equal to the number of bands where each element is the averaged value for the band
    with the same index.
    """

    # For the white/light reference images, we want to keep the x-axis within these constraints to account for
    # the difference between the image size and the size of the white reference card:
    WHITE_START = 60
    WHITE_END = 370

    rl = np.fromfile(rw_filepath, dtype=np.uint16)
    rl_reshaped = np.reshape(rl, (height * bands, width))

    # Take a slice of where the white reference card is expected to be:
    sliced = rl_reshaped[:, WHITE_START:WHITE_END]
    rl_averages = np.zeros(56)  # We will populate this array with the per-band averages

    for current_band in range(0, bands):
        # Slice every N rows in order to get the current band data:
        rl_band_arr = sliced[current_band::bands]
        rl_averages[current_band] = np.average(rl_band_arr)

    return rl_averages


def get_reference_dark_averages(rd_filepath=None, width=1024, height=30, bands=56):
    """ Gets hyperspectral reference "dark" data from the given file and returns an array of reflectance averages per band. """

    rd = np.fromfile(rd_filepath, dtype=np.uint16)
    rd_reshaped = np.reshape(rd, (height * bands, width))
    rd_averages = np.zeros(56)

    for current_band in range(0, bands):
        # Slice every N rows in order to get the current band data:
        rd_band_arr = rd_reshaped[current_band::bands]
        rd_averages[current_band] = np.average(rd_band_arr)

    return rd_averages


def calculate_reflectance(hyperspectral_data, light_average, dark_average):
    """ Takes hyperspectral data and adjusts the data based on the given light and dark values """
    a = 1.0 / (light_average - dark_average)
    b = -a * dark_average
    adjusted_data = (hyperspectral_data * a) + b
    adjusted_data = np.minimum(adjusted_data, 1.0)
    adjusted_data = np.maximum(adjusted_data, 0.0)
    return adjusted_data


def is_hyperspectral_file(fname):
    """ Identifies a filename as a hyperspectral subject file """
    fname_parser = EnviratronFileNameParser(fname)
    return fname_parser.type == "hyperspectral"


def is_hyperspectral_reference_file(fname):
    """ Identifies a filename as a hyperspectral reference file """

    # return fnmatch.fnmatch(fname, '?_hs_r*.bin')
    fname_parser = EnviratronFileNameParser(fname)
    return fname_parser.type in (
        "hyperspectral reference white",
        "hyperspectral reference dark",
    )


def collect_hyperspectral_files(root, files):
    """ Returns a list of EnviratronFileNameParser objects for the subset of arg files that has type == "hyperspectral" """
    file_paths = ["/".join([root, f]) for f in files]
    _mapper = lambda file_path: EnviratronFileNameParser(file_path)
    hyperspectral_files = [
        hs_f for hs_f in list(map(_mapper, file_paths)) if hs_f.type == "hyperspectral"
    ]
    return hyperspectral_files


def collect_hyperspectral_reference_files(root, files):
    """ Returns a list of EnviratronFileNameParser objects for the subset of arg files that has type == "hyperspectral reference..." """
    file_paths = ["/".join([root, f]) for f in files]
    _mapper = lambda file_path: EnviratronFileNameParser(file_path)
    dark_reference_files = [
        hs_f
        for hs_f in list(map(_mapper, file_paths))
        if hs_f.type == "hyperspectral reference dark"
    ]
    white_reference_files = [
        hs_f
        for hs_f in list(map(_mapper, file_paths))
        if hs_f.type == "hyperspectral reference white"
    ]
    return (white_reference_files, dark_reference_files)


def filter_reference_files(subject_file, white_reference_files, dark_reference_files):
    pass



def _log(*argv):
    ''' A logging function to possibly be fleshed out later. '''
    print(*argv)



def main(in_path, out_path, experiment_id, overwrite_existing=True, verbose=True):

    Path(out_path).mkdir(parents=True, exist_ok=True)

    for root, subdirs, files in os.walk(in_path):
        new_partial_path = root.split(f"{experiment_id}/")[-1]
        save_to_path = out_path + new_partial_path

        # There are some files we do not want to process:
        if "additional" in root:
            continue

        # The strategy is to loop through all the files and collect lists of subject files and reference files
        # Once the three lists are populated, we will loop the subject files list to identify the correct reference
        # files to use for each subject file
        subject_files = collect_hyperspectral_files(root, files)
        white_reference_files, dark_reference_files = collect_hyperspectral_reference_files(
            root, files
        )

        if len(subject_files) == 0:
            continue

        if verbose:
            _log("MAKING:\t\t", save_to_path)

        Path(save_to_path).mkdir(parents=True, exist_ok=True)

        assert len(dark_reference_files) > 0, f"There should be some dark reference hyperspectral files in the directory {root}"
        assert len(white_reference_files) > 0, f"There should be some white reference hyperspectral files in the directory {root}"

        # At this point we have a list of subject hyperspectral files and separate lists of dark and light
        # hyperspectral reference files. Now we will loop through the subject files and identify the
        # appropriate reference files to associate with each based on the timestamps in the filenames.
        # We want the ref files closest in time to the subject file, but not newer than the subject file.
        for subject_file in subject_files:

            np_out_path = (
                save_to_path + "/" + subject_file.raw_filename.replace(".bin", ".npy")
            )
            np_out_path_exists = os.path.exists(np_out_path)

            # If a particular file has already been processed and the overwrite_existing param is False, skip processing:
            if np_out_path_exists and not overwrite_existing:
                if verbose:
                    _log("SKIPPING:\t", subject_file.raw_filename)
                continue

            subject_file_datetime = arrow.get(subject_file.datetime)
            if verbose:
                _log("SUBJECT FILE:\t", subject_file.raw_filepath)

            # Filter the dark reference files to exclude any files newer than our subject file into a list of tuples of files and time deltas:
            find_dark_reference_file = [
                (f, subject_file_datetime - arrow.get(f.datetime))
                for f in dark_reference_files
                if arrow.get(f.datetime) < subject_file_datetime
            ]
            # Sort the list of reference files older than the subject file by time delta:
            find_dark_reference_file.sort(key=lambda tup: tup[1])
            # Finally, get the first file (closest in time, but not newer)
            try:
                dark_reference_file = find_dark_reference_file[0][0]
                if verbose:
                    _log("DARK REF FILE:\t", dark_reference_file.raw_filepath)
            except IndexError:
                if verbose:
                    _log("COULD NOT MATCH DARK REFERENCE FILE")
                    _log(subject_file.datetime)
                    _log("REFERENCE DARK FILES:")
                for dark_reference_file in dark_reference_files:
                    if verbose:
                        _log(
                            dark_reference_file.raw_filename, dark_reference_file.datetime
                        )

                continue

            # Now, do the same process to identify the white/light reference file
            find_white_reference_file = [
                (f, subject_file_datetime - arrow.get(f.datetime))
                for f in white_reference_files
                if arrow.get(f.datetime) < subject_file_datetime
            ]
            find_white_reference_file.sort(key=lambda tup: tup[1])

            try:
                white_reference_file = find_white_reference_file[0][0]
                if verbose:
                    _log("WHITE REF FILE:\t", white_reference_file.raw_filepath)
            except IndexError:
                if verbose:
                    _log("COULD NOT MATCH WHITE REFERENCE FILE")
                    _log(subject_file.datetime)
                    _log("REFERENCE WHITE FILES:")
                for white_reference_file in white_reference_files:
                    if verbose:
                        _log(
                            white_reference_file.raw_filename, white_reference_file.datetime
                        )
                continue

            # Read the reflectance reference data:
            reference_light_data = get_reference_light_averages(
                white_reference_file.raw_filepath
                , width=white_reference_file.width
                , height=white_reference_file.height
                , bands=white_reference_file.bands
            )
            reference_dark_data = get_reference_dark_averages(
                dark_reference_file.raw_filepath
                , width=dark_reference_file.width
                , height=dark_reference_file.height
                , bands=dark_reference_file.bands
            )
            # And read the subject file:
            hs_data = np.fromfile(subject_file.raw_filepath, dtype=np.uint16)

            try:
                # Here we calibrate/normalize the data one band at a time
                hs_data = np.reshape(
                    hs_data,
                    (subject_file.height * subject_file.bands, subject_file.width),
                )
                hs_data_adjusted = None

                # The source file has line-scan data, so each spectral band is spread throughout the data.
                # The first spectral band data is at rows 0, 55, 110, etc instead of 0, 1, 2, etc.
                # Here we "fix" that and arrange the data so each band is together

                for current_band in range(0, subject_file.bands):
                    band_arr = hs_data[current_band :: subject_file.bands]
                    band_arr_adjusted = calculate_reflectance(
                        band_arr,
                        reference_light_data[current_band],
                        reference_dark_data[current_band],
                    )

                    # Collect each band's data and stack the bands in an array with dimensions (bands, (width*height)):
                    if hs_data_adjusted is None:
                        hs_data_adjusted = np.copy(band_arr_adjusted)
                    else:
                        hs_data_adjusted = np.concatenate(
                            (hs_data_adjusted, band_arr_adjusted)
                        )

                if verbose:
                    _log("OUTPUTTING:\t", np_out_path)
                    _log("")
                np.save(np_out_path, hs_data_adjusted)

            except ValueError as v_err:
                # Probably can't fit the data into the expected width and height b/c there's a bad filename or bad file data:
                # Let's naively ignore this file for now:
                if verbose:
                    _log("*" * 50)
                    _log("ERROR")
                    _log(v_err)
                    _log(subject_file.raw_filename)
                    _log("*" * 50)
                pass


if __name__ == "__main__":
    ROOT_PATH = "c1_2019_5_27_7_6_50_692"
    OUT_PATH = "numpy_data/"
    EXPERIMENT_ID = 37
    main(ROOT_PATH, OUT_PATH, EXPERIMENT_ID, overwrite_existing=False, verbose=True)
